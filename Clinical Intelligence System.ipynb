{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b30033a",
   "metadata": {},
   "source": [
    "# Clinical Intelligence System - Capstone Project\n",
    "\n",
    "This notebook implements a Retrieval-Augmented Generation (RAG) pipeline for answering clinical questions using a trusted set of medical documents. It follows the capstone requirements and incorporates best practices from the provided training courses.\n",
    "\n",
    "## Steps:\n",
    "1. Load dataset\n",
    "2. Create embeddings and store in ChromaDB\n",
    "3. Explore multiple retrieval strategies\n",
    "4. Integrate with GPT model for generation\n",
    "5. Validate using evaluation dataset\n",
    "6. Test on unseen questions and save results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7535ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if running locally)\n",
    "# !pip install openai chromadb pandas langchain\n",
    "\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97867bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the main dataset\n",
    "rag_dataset = pd.read_csv('capstone1_rag_dataset.csv')\n",
    "print(f\"Dataset loaded with {len(rag_dataset)} documents.\")\n",
    "rag_dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bc9dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client and ChromaDB\n",
    "client = OpenAI()\n",
    "\n",
    "# Create ChromaDB client and collection\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name=\"medical_docs\")\n",
    "\n",
    "# Define embedding function using OpenAI's text-embedding-3-small\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(api_key=\"YOUR_API_KEY\", model_name=\"text-embedding-3-small\")\n",
    "\n",
    "# Add documents to ChromaDB\n",
    "for idx, row in rag_dataset.iterrows():\n",
    "    collection.add(documents=[row['document']], ids=[str(idx)], metadatas=[{\"source\": \"medical\"}])\n",
    "\n",
    "print(\"Documents added to ChromaDB.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8586fa6d",
   "metadata": {},
   "source": [
    "## Retrieval Strategy Exploration\n",
    "We will implement and compare multiple strategies:\n",
    "- Semantic Search\n",
    "- Semantic Search with Threshold Filtering\n",
    "- Hybrid Search (Keyword + Semantic)\n",
    "- Reranking based on relevance scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b0213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Semantic Search\n",
    "def semantic_search(query, top_k=3):\n",
    "    results = collection.query(query_texts=[query], n_results=top_k)\n",
    "    return results['documents'][0]\n",
    "\n",
    "print(semantic_search(\"What are symptoms of diabetes?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff2ef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold Filtering\n",
    "def semantic_search_with_threshold(query, top_k=5, threshold=0.75):\n",
    "    results = collection.query(query_texts=[query], n_results=top_k)\n",
    "    docs = results['documents'][0]\n",
    "    scores = results['distances'][0]\n",
    "    filtered_docs = [doc for doc, score in zip(docs, scores) if score >= threshold]\n",
    "    return filtered_docs\n",
    "\n",
    "# Hybrid Search\n",
    "def hybrid_search(query, top_k=5):\n",
    "    keyword_matches = [doc for doc in rag_dataset['document'] if re.search(query, doc, re.IGNORECASE)]\n",
    "    semantic_results = collection.query(query_texts=[query], n_results=top_k)\n",
    "    combined = list(set(keyword_matches[:top_k] + semantic_results['documents'][0]))\n",
    "    return combined[:top_k]\n",
    "\n",
    "# Reranking\n",
    "def rerank_search(query, top_k=5):\n",
    "    results = collection.query(query_texts=[query], n_results=top_k*2)\n",
    "    docs = results['documents'][0]\n",
    "    scores = results['distances'][0]\n",
    "    ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, _ in ranked[:top_k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba04c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect retriever with GPT model for generation\n",
    "def generate_answer(question, retrieved_docs):\n",
    "    context = \"\n",
    "\".join(retrieved_docs)\n",
    "    prompt = f\"You are a medical assistant. Answer the question based only on the context below.\n",
    "Context:\n",
    "{context}\n",
    "Question: {question}\n",
    "Answer:\"\n",
    "    response = client.chat.completions.create(model=\"gpt-4.1-mini\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2234d2a2",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "We will compute Precision@k, Recall@k, and F1-score for the validation dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f3ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(retrieved, relevant, k):\n",
    "    retrieved_k = retrieved[:k]\n",
    "    return len(set(retrieved_k) & set(relevant)) / len(retrieved_k) if retrieved_k else 0\n",
    "\n",
    "def recall_at_k(retrieved, relevant, k):\n",
    "    retrieved_k = retrieved[:k]\n",
    "    return len(set(retrieved_k) & set(relevant)) / len(relevant) if relevant else 0\n",
    "\n",
    "def f1_score(precision, recall):\n",
    "    return (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "validation_df = pd.read_csv('capstone1_rag_validation.csv')\n",
    "results = []\n",
    "for _, row in validation_df.iterrows():\n",
    "    question = row['question']\n",
    "    relevant_docs = row['reference_context'].split('|')\n",
    "    retrieved_docs = semantic_search(question)\n",
    "    p = precision_at_k(retrieved_docs, relevant_docs, k=3)\n",
    "    r = recall_at_k(retrieved_docs, relevant_docs, k=3)\n",
    "    f1 = f1_score(p, r)\n",
    "    results.append({\"question\": question, \"precision@3\": p, \"recall@3\": r, \"f1\": f1})\n",
    "\n",
    "metrics_df = pd.DataFrame(results)\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a91e5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv('capstone1_rag_test_questions.csv')\n",
    "results = []\n",
    "for _, row in submission_df.iterrows():\n",
    "    question = row['question']\n",
    "    retrieved_docs = semantic_search(question)\n",
    "    answer = generate_answer(question, retrieved_docs) if retrieved_docs else \"The question cannot be answered using the available documents.\"\n",
    "    results.append({\"question\": question, \"retrieved_documents\": retrieved_docs, \"generated_answer\": answer})\n",
    "final_df = pd.DataFrame(results)\n",
    "final_df.to_csv('submission.csv', index=False)\n",
    "print(\"submission.csv created successfully.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}